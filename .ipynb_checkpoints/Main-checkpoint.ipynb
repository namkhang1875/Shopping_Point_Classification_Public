{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# car_df is the dataframe load from veh_tx\n",
    "# df is the car_df dataframe that already group by, calculate the parking duration and assign shopping_point\n",
    "########################################################\n",
    "\n",
    "#################################################### STEP 1: Load Dataset #####################################################################\n",
    "\n",
    "car_df1 = pd.read_csv('G:\\\\My Drive\\\\ITS_DA\\\\veh_tx_data\\\\2019-11-10_15.zip', compression='zip')\n",
    "car_df2 = pd.read_csv('G:\\\\My Drive\\\\ITS_DA\\\\veh_tx_data\\\\2019-11-10_18.zip', compression='zip')\n",
    "\n",
    "#############################################################################################################################################\n",
    "\n",
    "#################################################### STEP 1.5: Data Processing ##############################################################\n",
    "# car_df1.isna().sum()\n",
    "# car_df2.isna().sum()\n",
    "car_df1.dropna(inplace=True)\n",
    "car_df2.dropna(inplace=True)\n",
    "\n",
    "car_df = pd.concat([car_df1,car_df2])\n",
    "# print(car_df.isna().sum())\n",
    "\n",
    "# print(car_df.info())\n",
    "# print(car_df.head(50))\n",
    "# print(shopping_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>vid</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>speed</th>\n",
       "      <th>unit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-10 15:00:11</td>\n",
       "      <td>C240E85CD6C30CAC3FEECF765FBECA30BDB6D9A6</td>\n",
       "      <td>16.147610</td>\n",
       "      <td>101.907509</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-10 15:00:19</td>\n",
       "      <td>D4C0E97A90CA51EEEE3D6A42A4E65D9234FBE5BA</td>\n",
       "      <td>15.273096</td>\n",
       "      <td>104.837271</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-10 15:00:00</td>\n",
       "      <td>08A279E01A70B1468F3FDC06C9139401F46BF2B9</td>\n",
       "      <td>14.188940</td>\n",
       "      <td>100.607180</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-11-10 15:00:00</td>\n",
       "      <td>08A279E01A70B1468F3FDC06C9139401F46BF2B9</td>\n",
       "      <td>14.188940</td>\n",
       "      <td>100.607180</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-10 15:00:05</td>\n",
       "      <td>2392305CF64954697BB3D47512DD3F5750FDCB75</td>\n",
       "      <td>12.250213</td>\n",
       "      <td>102.605698</td>\n",
       "      <td>60</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time_stamp                                       vid        lat  \\\n",
       "0  2019-11-10 15:00:11  C240E85CD6C30CAC3FEECF765FBECA30BDB6D9A6  16.147610   \n",
       "1  2019-11-10 15:00:19  D4C0E97A90CA51EEEE3D6A42A4E65D9234FBE5BA  15.273096   \n",
       "2  2019-11-10 15:00:00  08A279E01A70B1468F3FDC06C9139401F46BF2B9  14.188940   \n",
       "3  2019-11-10 15:00:00  08A279E01A70B1468F3FDC06C9139401F46BF2B9  14.188940   \n",
       "4  2019-11-10 15:00:05  2392305CF64954697BB3D47512DD3F5750FDCB75  12.250213   \n",
       "\n",
       "          lon  speed  unit_type  \n",
       "0  101.907509      0        8.0  \n",
       "1  104.837271      0        1.0  \n",
       "2  100.607180      0        5.0  \n",
       "3  100.607180      0        5.0  \n",
       "4  102.605698     60        7.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Time Elpased to detemine that which point is a shopping point\n",
    "car_df[\"time_stamp\"] = pd.to_datetime(car_df[\"time_stamp\"])\n",
    "\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "# Round Latitude Longitude to 1 digit float\n",
    "car_df[\"latx\"] = round(car_df[\"lat\"], 1)\n",
    "car_df[\"lonx\"] = round(car_df[\"lon\"], 1)\n",
    "\n",
    "# Round Latitude Longitude to 2 digit float\n",
    "car_df[\"latxx\"] = round(car_df[\"lat\"], 2)\n",
    "car_df[\"lonxx\"] = round(car_df[\"lon\"], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the value to calculate the duration of parking\n",
    "test = car_df[car_df['speed'] == 0].groupby(['vid','latxx','lonxx','unit_type'])['time_stamp'].min()  # first time_stamp \n",
    "test2 = car_df[car_df['speed'] == 0].groupby(['vid','latxx','lonxx','unit_type'])['time_stamp'].max() # last time_stamp\n",
    "ans = test2-test\n",
    "\n",
    "df1 = pd.DataFrame(data=ans.index, columns=['vid'])\n",
    "df2 = pd.DataFrame(data=ans.values, columns=['duration'])\n",
    "df = pd.merge(df1, df2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df[\"parking\"] = \"\"\n",
    "df[\"parking\"] = \"\"\n",
    "df[\"latxx\"] = \"\"\n",
    "df[\"lonxx\"] = \"\"\n",
    "df[\"unit_type\"] = \"\"\n",
    "df[['vid', 'latxx' ,'lonxx','unit_type']] = pd.DataFrame(df['vid'].tolist(),index=df.index)\n",
    "# print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# filter others province ############################## \n",
    "\n",
    "df.drop(df[(df.latxx < 18.72) | (df.lonxx <98.94)].index , inplace = True)\n",
    "df.drop(df[(df.latxx > 18.87) | (df.lonxx >99.06)].index , inplace = True)\n",
    "\n",
    "# print(df.head(30))\n",
    "\n",
    "############################ So now it has only points in Chiang Mai ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Loop to calculate the duration of parking #################################\n",
    "################ If it's a parking point then the column \"parking\" will be 1,  if not then will be 0 #####################\n",
    "list = []\n",
    "for row in df.itertuples():\n",
    "    time_stamp = row.duration\n",
    "    \n",
    "    seconds = time_stamp.seconds\n",
    "    hours = seconds//3600\n",
    "    minutes = (seconds//60) % 60\n",
    "\n",
    "    # print(\"minute = \", minutes)\n",
    "    # print(\"hours  = \", hours)\n",
    "    # print(\" \")\n",
    "\n",
    "    vid = row.vid\n",
    "    latx = row.latxx\n",
    "    lonx = row.lonxx\n",
    "    \n",
    "    indicator = 0\n",
    "\n",
    "    if (minutes >= 30 and hours < 1) or (hours >= 1 and hours < 2) or (hours >= 2 and minutes <= 30):  # 2 hours and half\n",
    "        indicator =1\n",
    "    else:\n",
    "        indicator =0\n",
    "       \n",
    "    list.append(indicator)\n",
    "    \n",
    "df['parking'] = list #assign parking indicator to dataframe\n",
    "\n",
    "\n",
    "\n",
    "# ตอนนี้ได้จุดจอดแล้ว เหลือนำไปสร้าง feature\n",
    "# แล้วก็กรองให้เหลือแต่เชียงใหม่แล้ว\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\Users\\\\Fantasticboy\\\\Documents\\\\TestDataAnalytic\\\\shoppingCenter4.csv' does not exist: b'C:\\\\Users\\\\Fantasticboy\\\\Documents\\\\TestDataAnalytic\\\\shoppingCenter4.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4fa61bbd5cb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m######################## Load Dataset of Shopping Point ###############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m shopping_df = pd.read_csv(\n\u001b[1;32m----> 3\u001b[1;33m     'C:\\\\Users\\\\Fantasticboy\\\\Documents\\\\TestDataAnalytic\\\\shoppingCenter4.csv')\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# print(shopping_df.head(30))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\Users\\\\Fantasticboy\\\\Documents\\\\TestDataAnalytic\\\\shoppingCenter4.csv' does not exist: b'C:\\\\Users\\\\Fantasticboy\\\\Documents\\\\TestDataAnalytic\\\\shoppingCenter4.csv'"
     ]
    }
   ],
   "source": [
    "######################## Load Dataset of Shopping Point ###############################\n",
    "shopping_df = pd.read_csv(\n",
    "    'C:\\\\Users\\\\Fantasticboy\\\\Documents\\\\TestDataAnalytic\\\\shoppingCenter4.csv')\n",
    "# print(shopping_df.head(30))\n",
    "\n",
    "df['shopping_point'] = \"\"\n",
    "shopping_list = []\n",
    "indicator2 = 0\n",
    "shopping_len = len(shopping_df['latxx'])\n",
    "iterator = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Compare coordinate of the dataframe and coordiante of the shopping point to assign the shopping point to column \"Shopping\" ##############\n",
    "\n",
    "for row in df.itertuples():\n",
    "    dflatxx = row.latxx\n",
    "    dflonxx = row.lonxx\n",
    "    dfunit_type = row.unit_type\n",
    "    dfparking = row.parking\n",
    "    for a,b in zip(shopping_df.latxx, shopping_df.lonxx):\n",
    "        if dflatxx == a and dflonxx == b :\n",
    "            shopping_list.append(1)\n",
    "            iterator = 0\n",
    "            break\n",
    "        \n",
    "        iterator +=1\n",
    "        if iterator == shopping_len:\n",
    "            shopping_list.append(0)\n",
    "        \n",
    "    iterator = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# If the length of 2 dataframe is not match then fill by 0 #################\n",
    "df['shopping_point'] = shopping_list\n",
    "# print(df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "df['is_has_a_lot_of_cars'] = \"\"\n",
    "# temp_df['number_of_cars'] = \"\"\n",
    "# temp_df = df.groupby(['latxx','lonxx'])\n",
    "temp = df[df['parking'] == 1].groupby(['latxx','lonxx'])['vid'].count()\n",
    "temp_df1 = pd.DataFrame(data=temp.values, columns=['count'])\n",
    "temp_df2 = pd.DataFrame(data=temp.index, columns=['latxx_lonxx'])\n",
    "temp_df = pd.merge(temp_df1,temp_df2, left_index=True, right_index=True)\n",
    "# temp_df = df[['vid','parking','latxx','lonxx']]\n",
    "# print(temp_df1.head())\n",
    "# print(temp_df2.head())\n",
    "# print(temp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### Calculate the point that has a lot of cars ###########################\n",
    "df['is_has_a_lot_of_cars'] = \"\"\n",
    "# temp_df['number_of_cars'] = \"\"\n",
    "# temp_df = df.groupby(['latxx','lonxx'])\n",
    "temp = df[df['parking'] == 1].groupby(['latxx','lonxx'])['vid'].count()\n",
    "temp_df1 = pd.DataFrame(data=temp.values, columns=['count'])\n",
    "temp_df2 = pd.DataFrame(data=temp.index, columns=['latxx_lonxx'])\n",
    "temp_df = pd.merge(temp_df1,temp_df2, left_index=True, right_index=True)\n",
    "# temp_df = df[['vid','parking','latxx','lonxx']]\n",
    "# print(temp_df1.head())\n",
    "# print(temp_df2.head())\n",
    "# print(temp_df.head())\n",
    "\n",
    "\n",
    "temp_df[\"latxx\"] = \"\"\n",
    "temp_df[\"lonxx\"] = \"\"\n",
    "\n",
    "temp_df[[ 'latxx' ,'lonxx']] = pd.DataFrame(temp_df['latxx_lonxx'].tolist(),index=temp_df.index)\n",
    "# df[['vid', 'latxx' ,'lonxx','unit_type']] = pd.DataFrame(df['vid'].tolist(),index=df.index)\n",
    "# print(temp_df.head(10))\n",
    "\n",
    "iterator2 = 0\n",
    "tempdf_len = len(temp_df['latxx'])\n",
    "\n",
    "for a,b in zip(df.latxx, df.lonxx):\n",
    "    \n",
    "    for row in temp_df.itertuples():\n",
    "        count=row.count\n",
    "        temp_latxx = row.latxx\n",
    "        temp_lonxx = row.lonxx\n",
    "            \n",
    "        if temp_latxx == a and temp_lonxx == b:\n",
    "            if count >=15:\n",
    "                # df[(df['latxx'] == a) & (df['lonxx'] == b)]['is_has_a_lot_of_cars'] = 1\n",
    "                df.loc[(df['latxx'] == a) & (df['lonxx'] == b), 'is_has_a_lot_of_cars'] = 1\n",
    "                iterator2 = 0\n",
    "                break\n",
    "            \n",
    "        iterator2 +=1\n",
    "        if iterator2 == tempdf_len:\n",
    "            # df[(df['latxx'] == a) & (df['lonxx'] == b)]['is_has_a_lot_of_cars'] = 0\n",
    "            df.loc[(df['latxx'] == a) & (df['lonxx'] == b), 'is_has_a_lot_of_cars'] = 0\n",
    "            iterator2 = 0\n",
    "\n",
    "# print(df.head(20))\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"shopping_point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"parking\", hue=\"shopping_point\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"unit_type\", hue=\"shopping_point\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"is_has_a_lot_of_cars\", hue=\"shopping_point\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"shopping_point\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('shopping_point').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().sort_values(\"shopping_point\")[\"shopping_point\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, x_vars=['parking','unit_type','is_has_a_lot_of_cars'], y_vars=['shopping_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Feature Selection ###################################\n",
    "X = df[['parking','unit_type','is_has_a_lot_of_cars']]\n",
    "Y = df['shopping_point']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Split the train and test dataset ########################\n",
    "# print(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=101)\n",
    "\n",
    "# print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################## Build a model ####################################\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# # clf = LogisticRegression()\n",
    "# # clf.fit(X, Y)\n",
    "\n",
    "# # clf = tree.DecisionTreeClassifier()\n",
    "# # clf.fit(X, Y)\n",
    "\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf.fit(X_train, Y_train)\n",
    "\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(X_train,Y_train)\n",
    "\n",
    "# # clf = svm.SVC()\n",
    "# # clf.fit(X, Y)\n",
    "\n",
    "# clf = KNeighborsClassifier(n_neighbors=5)\n",
    "# clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Predict #####################\n",
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Report #####################\n",
    "print(\"*********Confusion Matrix*********\")\n",
    "cm_labels = df['shopping_point'].unique()\n",
    "print(cm_labels)\n",
    "print(confusion_matrix(Y_test, Y_pred, labels = cm_labels))\n",
    "\n",
    "print(\"**************Report**************\")\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "print(\"************* F1 ***************\")\n",
    "f1 = f1_score(Y_test, Y_pred, average = 'weighted')\n",
    "print('F1 = ', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
